{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b3c557bc-11f3-4724-ab6e-b6a2f090aa84","_cell_guid":"da616f3f-154c-4d40-983b-3b5756eddeac","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:32.444942Z","iopub.execute_input":"2023-06-12T08:35:32.445263Z","iopub.status.idle":"2023-06-12T08:35:32.463142Z","shell.execute_reply.started":"2023-06-12T08:35:32.445234Z","shell.execute_reply":"2023-06-12T08:35:32.462075Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Generation using LSTM \n\nThis is actually a fun project, since we are generating cute baby names using RNN \n\nThe credit for the code goes to : **Alladin Persson** \n_aladin.persson@hotmail.com_","metadata":{"_uuid":"57523f26-3592-4e14-ba82-63a3af82d39f","_cell_guid":"0a8c9082-53ef-422c-a898-3ee2c7730151","trusted":true}},{"cell_type":"code","source":"# Importing dependencies for the project\n\nimport torch \nimport torch.nn as nn \nimport string \nimport re \nimport random \nimport unidecode","metadata":{"_uuid":"6d87e377-14f5-4000-8a39-12ce8dd6ab42","_cell_guid":"6b5e9a0b-aa4d-45ce-926d-8b21b2673cfd","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:32.465924Z","iopub.execute_input":"2023-06-12T08:35:32.466253Z","iopub.status.idle":"2023-06-12T08:35:37.382991Z","shell.execute_reply.started":"2023-06-12T08:35:32.466223Z","shell.execute_reply":"2023-06-12T08:35:37.381904Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"_uuid":"b2b00708-0cb9-4395-ac95-4cb03db15433","_cell_guid":"d8b78c53-1f5a-48f9-a9c0-d37664317d4e","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:37.384478Z","iopub.execute_input":"2023-06-12T08:35:37.385394Z","iopub.status.idle":"2023-06-12T08:35:37.515513Z","shell.execute_reply.started":"2023-06-12T08:35:37.385361Z","shell.execute_reply":"2023-06-12T08:35:37.514253Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_characters = string.printable\nn_characters = len(all_characters)\nn_characters","metadata":{"_uuid":"a11904bf-81d9-46ec-8686-c4216e397416","_cell_guid":"4317e983-baad-4d4b-875f-de8746128df5","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:37.518020Z","iopub.execute_input":"2023-06-12T08:35:37.519648Z","iopub.status.idle":"2023-06-12T08:35:37.528004Z","shell.execute_reply.started":"2023-06-12T08:35:37.519611Z","shell.execute_reply":"2023-06-12T08:35:37.526661Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = unidecode.unidecode(open(\"/kaggle/input/babynames/names.txt\").read())\n# file","metadata":{"_uuid":"8d9534f3-021b-434d-9849-c13458db5e3b","_cell_guid":"5f4cd3c7-ba64-46dc-8e46-4546f15037b5","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:37.534735Z","iopub.execute_input":"2023-06-12T08:35:37.536224Z","iopub.status.idle":"2023-06-12T08:35:42.356548Z","shell.execute_reply.started":"2023-06-12T08:35:37.536172Z","shell.execute_reply":"2023-06-12T08:35:42.355469Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN class","metadata":{"_uuid":"0d4723cc-c145-40cb-a480-1dca1995ccc5","_cell_guid":"1dff0a8e-3c12-4880-aceb-270eb2bfabf6","trusted":true}},{"cell_type":"code","source":"class RNN(nn.Module):\n    \n    def __init__(self , input_size , hidden_size ,num_layers, output_size):\n        super().__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.num_layers = num_layers\n        \n        self.embed = nn.Embedding(input_size ,hidden_size)\n        self.lstm = nn.LSTM(hidden_size , hidden_size , num_layers , batch_first = True)\n        self.fc = nn.Linear(hidden_size , output_size)\n        \n    def forward(self,x, hidden, cell):\n        out = self.embed(x)\n        out , (hidden,cell) = self.lstm(out.unsqueeze(1) , (hidden,cell ))\n        out = self.fc(out.reshape(out.shape[0] , -1))\n        return out, (hidden ,cell)\n    \n    def init_hidden(self,batch_size):\n        hidden = torch.zeros(self.num_layers , batch_size , self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers , batch_size , self.hidden_size).to(device)\n        \n        return hidden , cell","metadata":{"_uuid":"b5725803-f2ff-425e-b9b5-29f827ebbe5d","_cell_guid":"fe82634c-5e15-46df-bea9-6c13bee65730","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:42.357898Z","iopub.execute_input":"2023-06-12T08:35:42.358343Z","iopub.status.idle":"2023-06-12T08:35:42.368106Z","shell.execute_reply.started":"2023-06-12T08:35:42.358310Z","shell.execute_reply":"2023-06-12T08:35:42.367026Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generator class","metadata":{"_uuid":"9d61db36-a01d-43e5-9c28-1b1b49ad037c","_cell_guid":"c5de9111-ca28-4604-8e2c-23f6233761c6","trusted":true}},{"cell_type":"code","source":"class Generator: \n    \n    def __init__(self):\n        self.chunk_len = 250\n        self.epochs = 1000\n        self.batch_size = 1\n        self.print_every = 50\n        self.hidden_size = 256 \n        self.num_layers = 2\n        self.lr = 0.003 \n        \n    def char_tensor(self , string):\n        \n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = all_characters.index(string[c])\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len +1 \n        text_str = file[start_idx : end_idx]\n        \n        text_input = torch.zeros(self.batch_size , self.chunk_len)\n        text_target = torch.zeros(self.batch_size , self.chunk_len)\n        \n        for i in range(self.batch_size):\n            text_input[i,:] = self.char_tensor(text_str[:-1])\n            text_target[i,:] = self.char_tensor(text_str[1:])\n            \n        return text_input.long() , text_target.long()\n    \n    def generate(self , initial_str = \"A\" , predict_len = 100 , temperature = 0.85):\n        \"\"\"\n        :params \n        initial_str :str ,\n        predict_len : int ,\n        temperature : float, \n        \"\"\"\n        hidden, cell = self.rnn.init_hidden(batch_size = self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str \n        \n        for p in range(len(initial_str) - 1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    def train(self):\n        \n        self.rnn = RNN(\n            n_characters , \n            self.hidden_size , \n            self.num_layers,  \n            n_characters\n        ).to(device)\n        optimizer = torch.optim.Adam(self.rnn.parameters() , lr = self.lr)\n        criterion = nn.CrossEntropyLoss()\n#         writer  =\n        \n        print(\"=== Started Training ===\")\n        for epoch in range(self.epochs+1):\n            inp , target = self.get_random_batch()\n            hidden, cell = self.rnn.init_hidden(self.batch_size)\n            \n            self.rnn.zero_grad()\n            loss = 0 \n            inp = inp.to(device)\n            target = target.to(device)\n            \n            for c in range(self.chunk_len):\n                output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                loss += criterion(output, target[:, c])\n\n            loss.backward()\n            optimizer.step()\n            loss = loss.item() / self.chunk_len\n            \n            if epoch % self.print_every ==0:\n                print(f\"Loss : {loss}\")\n                print(self.generate())","metadata":{"_uuid":"d9ec3bf0-c1ae-450a-b0ef-7f00cda11811","_cell_guid":"8f5a0c37-ddd0-482b-ba75-fae891bf2524","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:42.369638Z","iopub.execute_input":"2023-06-12T08:35:42.370227Z","iopub.status.idle":"2023-06-12T08:35:42.392477Z","shell.execute_reply.started":"2023-06-12T08:35:42.370194Z","shell.execute_reply":"2023-06-12T08:35:42.391456Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gennames = Generator()\ngennames.train()","metadata":{"_uuid":"1c674b4c-4e0e-4a2c-87f2-6f1b19fc7001","_cell_guid":"68b4d4e9-c3f0-4168-bb84-8741a01798e4","collapsed":false,"execution":{"iopub.status.busy":"2023-06-12T08:35:42.394797Z","iopub.execute_input":"2023-06-12T08:35:42.395354Z","iopub.status.idle":"2023-06-12T08:39:43.064690Z","shell.execute_reply.started":"2023-06-12T08:35:42.395312Z","shell.execute_reply":"2023-06-12T08:39:43.063495Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"9f47eeba-30c4-465c-8db8-fe1f99d8de46","_cell_guid":"de0dcc88-a503-43e8-9461-3dfb13201707","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"1c101814-0dfb-4af5-bc8f-280e55939131","_cell_guid":"31803239-1387-403c-af84-6fc96a8c236e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}